{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GJn640uKI0T"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "import collections\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "class SuperClassifier:\n",
        "  #Our MultiLayer Perceptron classifier class. Represents a neural network with a feedforward classifier\n",
        "\n",
        "  #Constructor class, where we set our hyperparameters (learning rate and number of epoches)\n",
        "  #We also set up the layers, weights, biases and loss\n",
        "  def __init__(self, learningRate : float, epoches: int):\n",
        "    self.learningRate = learningRate\n",
        "    self.epoches = epoches\n",
        "    self.layers = [3,5,1] #3 input layers, 5 hidden layers and 1 output layer\n",
        "\n",
        "    #Set weights, biases and losses to empty arrays that we'll populate later\n",
        "    self.weights = []\n",
        "    self.biases = []\n",
        "    self.loss = []\n",
        "\n",
        "  #LOSS AND SIGMOID FUNCTIONS\n",
        "\n",
        "  def ourLossFun(self, y_trueVals : np.array, y_predVals : np.array) -> np.array:\n",
        "    #Our loss function, takes a numPy array of the true values, a numPy array of the predicted values, then returns a numPy array\n",
        "    #Current loss function: Cross-Entropy\n",
        "    #TODO: Gabe put in another loss function also remove this comment\n",
        "    return (y_trueVals * -1) * np.log2(y_predVals) - (1 - y_trueVals) * np.log2(1 - y_predVals)\n",
        "\n",
        "  def sigmoid(self, dotProd: np.array) -> np.array:\n",
        "    #Our function to calculate the sigmoid activation\n",
        "    #Takes the dot product of weights and inputs, with the bias added on (similar to what we did in the labs)\n",
        "    #The output is our activation value\n",
        "\n",
        "    #np.exp gets the exponential of each array element. For each element x we do x^e, where e is Euler's number\n",
        "    return 1/(1+np.exp(-dotProd))\n",
        "\n",
        "  #DERIVATIONS OF LOSS AND SIGMOID FUNCTIONS\n",
        "\n",
        "  def deriveLossFun(self, y_trueVals : np.array, y_predVals : np.array) -> np.array:\n",
        "    #The derivation of cross-entropy (not sure if necessary)\n",
        "    #This is the result of the same derivation from our lecture slides!\n",
        "\n",
        "    return -(1/np.log(2)) * ( (y_trueVals/y_predVals) - ( (1-y_trueVals) / (1-y_predVals) ) )\n",
        "\n",
        "  def deriveSigmoid(self, dotProd: np.array) -> np.array:\n",
        "    #The derivation of our sigmoid function\n",
        "    #dotProd is the same input as our normal sigmoid function\n",
        "\n",
        "    return self.sigmoid(dotProd)*(1- self.sigmoid(dotProd))\n",
        "\n",
        "  #FORWARD PASS, BACK PROPOGATION AND WEIGHT UPDATING\n",
        "\n",
        "  def forwardPass(self, X: np.array):# -> Tuple[List[np.array], List[np.array]]: Not hard-setting the return value as Python is throwing a warning for Tuples\n",
        "\n",
        "    #Our forward pass function, similar to what we've seen in lectures\n",
        "    #Input is an array with predicted values\n",
        "    #We output a list of activations and a list of derivations of each activation for every layer\n",
        "\n",
        "    #Input layer!\n",
        "    inputToLayer = np.copy(X) #Copy the predicted values to a new variable\n",
        "    activations = [inputToLayer] #Put this into a regular array\n",
        "    derivatives = [np.zeros(X.shape)] #Creates a new numPy array the dimensions of X, but every element is 0. This is put in a regular array.\n",
        "\n",
        "    #Hidden layer!\n",
        "    dotProd = np.matmul(self.weights[1], inputToLayer) + self.biases[1] #Gets the dot product and turns it into a sigmoid, similar to our labs\n",
        "    inputToLayer = self.sigmoid(dotProd) #Get a sigmoid of this\n",
        "    activations.append(inputToLayer) #Add this to our activations\n",
        "    derivatives.append(self.deriveSigmoid(dotProd)) #Add the derivative to our derivatives (TODO: Learn how to write better comments)\n",
        "\n",
        "    return(activations, derivatives)\n",
        "\n",
        "  def backpropogation(self, activations: List[np.array], derivatives : List[np.array], y: np.array):\n",
        "    #This function will pass drivatives with losses back through the network\n",
        "\n",
        "    #Get our accumulative loss for this layer and add it to the global array for passing back\n",
        "    self.loss.append( (1/y.shape[1])*np.sum( self.ourLossFun(y, activations[-1]) ) )\n",
        "\n",
        "    #Output layer derivations\n",
        "    dl_dy2 = self.deriveLossFun(y, activations[2])\n",
        "    dl_dz2 = np.multiply(dl_dy2, derivatives[2])\n",
        "    dl_dw2 = ( 1/y.shape[1] )*np.matmul(dl_dz2, activations[1].T)\n",
        "    dl_db2 = ( 1/y.shape[1] )*np.sum(dl_dz2, axis = 1)\n",
        "\n",
        "    #Hidden layer derivations\n",
        "    dl_dy1 = np.matmul(self.weights[1].T, dl_dz2)\n",
        "    dl_dz1 = np.multiply(dl_dy1, derivatives[1])\n",
        "    dl_dw1 = ( 1/y.shape[1] )*np.matmul(dl_dz1, activations[0].T)\n",
        "    dl_db1 = ( 1/y.shape[1] )*np.sum(dl_dz1, axis = 1)\n",
        "\n",
        "    #Return the loss derivatives for the weights and biases\n",
        "    return([dl_dw1, dl_dw2], [dl_db1, dl_db2])\n",
        "\n",
        "  def updateWeights(self, derivedWeights: List[np.array], derivedBias: List[np.array]):\n",
        "    #Update the weights using our derived losses and the MLP learning rule\n",
        "\n",
        "    self.weights[0] -= self.learningRate*derivedWeights[0]\n",
        "    self.weights[1] -= self.learningRate*derivedWeights[1]\n",
        "    self.biases[0] -= self.learningRate*derivedBias[0].reshape(-1, 1)\n",
        "    self.biases[1] -= self.learningRate*derivedBias[1].reshape(-1, 1)\n",
        "\n",
        "  def trainIt(self, X: np.array, y:np.array):\n",
        "    #This function, usually called 'fit' in these types of programs, trains the model\n",
        "\n",
        "    #X is an array of predicted values and their shape\n",
        "    #y is an array of target values and their shape\n",
        "\n",
        "    #Initialise the weights, biases and losses by resetting them\n",
        "    self.weights.clear()\n",
        "    self.biases.clear()\n",
        "    self.loss.clear()\n",
        "\n",
        "    #Randomize the elements in the arrays\n",
        "    for index in range(len(self.layers)-1):\n",
        "      self.weights.append( np.random.randn( self.layers[index+1], self.layers[index] ) * 0.1 )\n",
        "      self.biases.append( np.random.randn( self.layers[index+1], 1 ) * 0.1 )\n",
        "\n",
        "    #Now adjust the weights for each epoch\n",
        "    for unused in range(self.epoches):\n",
        "      #Run the forward pass\n",
        "      activations, derivations = self.forwardPass(X.T)\n",
        "      #Run the backpropogation\n",
        "      deriveWeight, deriveBias = self.backpropogation(activations, derivations, y.T)\n",
        "      self.updateWeights(deriveWeight, deriveBias)\n",
        "\n",
        "  def predictFun(self, X: np.array) -> np.array:\n",
        "    #Get the predicted values using a trained MLP class as a NumPy array\n",
        "\n",
        "    activations, _ = self.forwardPass(X.T) # We only want the activations here, _ lets us put the derivatives into a null variable, essentially deleting them\n",
        "    return np.rint(activations[2]).reshape(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data_loader.py\n",
        "\n",
        "import pandas as pd\n",
        "import pickle as cPickle\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    #open file\n",
        "    f = open('./data/data_cleaned.csv', 'r')\n",
        "    #split data\n",
        "    training_data, validation_data, test_data, garbage_data = cPickle.load(f, encoding='latin')\n",
        "    #close file\n",
        "    f.close()\n",
        "\n",
        "    #make one of the subsets noisy for fun\n",
        "    df = pd.DataFrame(garbage_data)\n",
        "    noise = np.random.normal(0,0.4, garbage_data.shape)\n",
        "    garbage_data = garbage_data + noise\n",
        "\n",
        "    #return as list\n",
        "    return list((training_data, validation_data, test_data, garbage_data))"
      ],
      "metadata": {
        "id": "E1F1VfNaMdBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data_cleaner.py\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "#load dataset\n",
        "data = pd.read_csv('data/data.csv')\n",
        "scatter_matrix(data, alpha=0.5, figsize=(20, 20)) #Setting up a scatter plot of the initial data\n",
        "\n",
        "#clean data\n",
        "data.drop(columns=['VIN (1-10)'], inplace=True)\n",
        "data['County'] = data['County'].fillna(data['County'].mode()[0])\n",
        "data['City'] = data['City'].fillna(data['City'].mode()[0])\n",
        "data.dropna(subset=['Postal Code'], inplace=True)\n",
        "data['Model'] = data['Model'].fillna(data['Model'].mode()[0])\n",
        "data['Electric Range'] = data['Electric Range'].fillna(data['Electric Range'].median())\n",
        "data['Legislative District'] = data['Legislative District'].fillna(data['Legislative District'].mode()[0])\n",
        "data['Electric Utility'] = data['Electric Utility'].fillna(data['Electric Utility'].mode()[0])\n",
        "data['2020 Census Tract'] = data['2020 Census Tract'].fillna(data['2020 Census Tract'].mode()[0])\n",
        "data.drop(columns=['2020 Census Tract'], inplace=True)\n",
        "data.drop(columns=['DOL Vehicle ID'], inplace=True)\n",
        "\n",
        "#converting categorical data to numeric data\n",
        "label_encoder = LabelEncoder()\n",
        "data['E.V_Type'] = label_encoder.fit_transform(data['E.V_Type'])\n",
        "data['County'] = label_encoder.fit_transform(data['County'])\n",
        "data['City'] = label_encoder.fit_transform(data['City'])\n",
        "data['State'] = label_encoder.fit_transform(data['State'])\n",
        "data['Make'] = label_encoder.fit_transform(data['Make'])\n",
        "data['Model'] = label_encoder.fit_transform(data['Model'])\n",
        "data['CAFV'] = label_encoder.fit_transform(data['CAFV'])\n",
        "data['Vehicle Location'] = label_encoder.fit_transform(data['Vehicle Location'])\n",
        "data['Electric Utility'] = label_encoder.fit_transform(data['Electric Utility'])\n",
        "data['County'] = label_encoder.fit_transform(data['County'])\n",
        "\n",
        "\n",
        "#export cleaned data\n",
        "data.to_csv('data/data_cleaned.csv', index = False)"
      ],
      "metadata": {
        "id": "yVL37-VuIvO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We plot our loss with this function. The inputs are our hyperparameters, letting us modify them easily and check for overfitting\n",
        "def plotFunction(lr : float, ec : int):\n",
        "\n",
        "  #lr is the Learning Rate\n",
        "  #ec is the number of Epoches\n",
        "\n",
        "  dataList = load_data()\n",
        "  X_train, y_train, X_test, y_test = dataList[0], dataList[1], dataList[2], dataList[3]\n",
        "\n",
        "  model = SuperClassifier(learningRate=lr, epoches=ec)\n",
        "  model.trainIt(X_train, y_train)\n",
        "\n",
        "  #Now to plot the loss...\n",
        "  plt.plot(model.loss)\n",
        "  plt.title(\"Training Loss\")\n",
        "  plt.xlabel(\"epochs = \", ec)\n",
        "  plt.ylabel(\"loss = \", lr)\n",
        "  plt.show()\n",
        "\n",
        "plotFunction(0.4, 50) #Standard hyperparameters\n",
        "plotFunction(0.1, 50) #Very small learning rate\n",
        "plotFunction(0.95, 70) #Very large learning rate and more epochs\n",
        "plotFunction(0.4, 200) #Normal learning rate, large number of epochs"
      ],
      "metadata": {
        "id": "pBK-DpFspjw3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}